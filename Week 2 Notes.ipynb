{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32fe6780-b689-4cbc-aef4-4525b62f3c99",
   "metadata": {},
   "source": [
    "# Keras TextVectorization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777664c-0ebc-46ca-a034-3bae211b713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment if you have MacOS sonoma and get an error running \"count_vectorizer.adapt(ds_texts)\"\n",
    "# # Source: https://discuss.tensorflow.org/t/upgrading-os-to-sonoma-on-my-mac-causing-tensorflow-errors/19846/5\n",
    "# import tensorflow as tf\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12ab02-c8ef-4a9a-9a6a-d66a748f459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Then Set Random Seeds\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# Then run the Enable Deterministic Operations Function\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903c006-4cf6-45b9-b2da-93a6efd1cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597223c-233a-4f8a-9f86-7e0366cdd31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from your file path\n",
    "df_demo = pd.read_csv(\"Data/programming-or-data-science.csv\")\n",
    "df_demo.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7115cc34-94aa-4d70-a682-d4060052ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = df_demo['text']\n",
    "y_string= df_demo['label']\n",
    "# Instantiate the LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "# Fit and Transform the strings into integers\n",
    "y = pd.Series(encoder.fit_transform(y_string))\n",
    "y.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37392b67-3e1f-4db3-9fdc-e8db9fdeb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dataset object\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1da505-c5f7-4e28-8343-cf647b7dbee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the data once\n",
    "ds = ds.shuffle(buffer_size=len(ds), reshuffle_each_iteration=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1219c-5198-45dd-b2d5-51c97af5fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determing how many samples for each split\n",
    "# Calculate the number of samples for training \n",
    "split_train = 0.7\n",
    "n_train_samples =  int(len(ds) * split_train)\n",
    "print(f\"Use {n_train_samples} samples as training data\")\n",
    "# Calculate the number of samples for validation\n",
    "split_val = 0.2\n",
    "n_val_samples = int(len(ds) * split_val)\n",
    "print(f\"Use {n_val_samples} samples as validation data\")\n",
    "# Test size is remainder\n",
    "split_test = 1 - (split_train + split_val)\n",
    "print(f\"The remaining {len(ds)- (n_train_samples+n_val_samples)} samples will be used as test data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5b02cc-4033-4b6a-9307-5cc477224310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .take to slice out the number of samples for training\n",
    "train_ds = ds.take(n_train_samples)\n",
    "# Skipover the training batches\n",
    "val_ds = ds.skip(n_train_samples)\n",
    "# Take .take to slice out the correct number of samples for validation\n",
    "val_ds = val_ds.take(n_val_samples)\n",
    "# Skip over all of the training + validation samples, the rest remain as samples for testing\n",
    "test_ds = ds.skip(n_train_samples + n_val_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46068d07-ff20-49d0-9a8e-bb034bd4fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffling just the training data  \n",
    "train_ds  = train_ds.shuffle(buffer_size = len(train_ds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e502ac-daeb-4d12-ad19-48600824c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Setting the batch_size for all datasets\n",
    "BATCH_SIZE = 1\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "val_ds = val_ds.batch(BATCH_SIZE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\n",
    "# Confirm the number of batches in each\n",
    "print (f' There are {len(train_ds)} training batches.')\n",
    "print (f' There are {len(val_ds)} validation batches.')\n",
    "print (f' There are {len(test_ds)} testing batches.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d402fd9-6a40-44e4-b705-eefc6da6a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a sample \n",
    "example_X, example_y= train_ds.take(1).get_single_element()\n",
    "print(example_X)\n",
    "print(example_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f67f20-7f44-4a8f-bca2-ade5a2f3e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the text from ds_train\n",
    "ds_texts = train_ds.map(lambda x, y: x)\n",
    "# Preview the text\n",
    "ds_texts.take(1).get_single_element()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d53b1a-3851-4ddd-925a-ff6eba7b9227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TextVectorization layer\n",
    "count_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_mode=\"count\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b39463-5dc3-410b-8414-260e65717f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training, only contains the out of vocab token ([UNK])\n",
    "count_vectorizer.get_vocabulary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4471f7-9e0e-4d03-ba45-eba8493991d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the layer on the training texts\n",
    "count_vectorizer.adapt(ds_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd739c38-d191-43ad-acc1-37d67a6ab462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of vocab\n",
    "vocab = count_vectorizer.get_vocabulary()\n",
    "# Exploring list of vocab\n",
    "type(vocab), len(vocab), vocab[:6]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4aae80-80a2-49e4-8209-9dc58114d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first value will be the count of all of the words not in the vocobulary\n",
    "counts= count_vectorizer(['python python python python is the most amazing thing in the world for data science!'])\n",
    "counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8194efa6-c3c6-4058-a680-b847321d609f",
   "metadata": {},
   "source": [
    "## TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002606f4-d035-4798-a813-82b1fa7f9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Text Vectorization Layer\n",
    "tfidf_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_mode=\"tf_idf\",\n",
    ")\n",
    "# Build the vectorizer vocabulary\n",
    "tfidf_vectorizer.adapt(ds_texts)\n",
    "# Confrim vocabulary size\n",
    "tfidf_vectorizer.vocabulary_size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f311d3-cfb4-4a1e-b7c1-6a3b813007db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first value will be the score of all of the words not in the vocobulary\n",
    "tfidf= tfidf_vectorizer(['python python python python is the most amazing thing in the world for data science!'])\n",
    "tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972406e-b85a-4311-92f9-53340c0420ca",
   "metadata": {},
   "source": [
    "## Sequence Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219cfae-9f4f-4f83-9a3d-d771cedffea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text Vectorization layer\n",
    "sequence_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=30\n",
    ")\n",
    "sequence_vectorizer.adapt(ds_texts)\n",
    "sequence_vectorizer.vocabulary_size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59978bae-5842-4978-82a7-826852b45575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the sequence of sample text with the sequence_vectorizer\n",
    "sequence= sequence_vectorizer(['python python python python is the most amazing thing in the world for data science!'])\n",
    "sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f8cc3-11fd-449b-920c-11bbddd6ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of vocab\n",
    "vocab = sequence_vectorizer.get_vocabulary()\n",
    "int_to_str = {idx: word for idx, word in enumerate(vocab)}\n",
    "int_to_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee64f91-7ae4-4be7-be13-d6fdf103849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What term corresponds to 94?\n",
    "int_to_str[94]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961b130-071c-479f-946f-32229bf8d13f",
   "metadata": {},
   "source": [
    "# Python File for Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968499f-1a6a-4f2b-a1a3-797a5511e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a demo custom function for .py file\n",
    "def demo_function(name):\n",
    "    print(f'Hello, {name}!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ba974-a657-496c-9bbe-1180928e7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "​## Load the autoreload extension\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import custom_functions_lp as fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f773f-d330-403e-98b4-cdd2f9297cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call custom function from imported file\n",
    "fn.demo_function('Brenda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e3bd9-f2ae-418e-b6e4-ac9dd90efd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREVIOUS CLASSIFICATION_METRICS FUNCTION FROM INTRO TO ML\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def classification_metrics(y_true, y_pred, label='',\n",
    "                           output_dict=False, figsize=(8,4),\n",
    "                           normalize='true', cmap='Blues',\n",
    "                           colorbar=False,values_format=\".2f\"):\n",
    "    \"\"\"Modified version of classification metrics function from Intro to Machine Learning.\n",
    "    Updates:\n",
    "    - Reversed raw counts confusion matrix cmap  (so darker==more).\n",
    "    - Added arg for normalized confusion matrix values_format\n",
    "    \"\"\"\n",
    "    # Get the classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    \n",
    "    ## Print header and report\n",
    "    header = \"-\"*70\n",
    "    print(header, f\" Classification Metrics: {label}\", header, sep='\\n')\n",
    "    print(report)\n",
    "    \n",
    "    ## CONFUSION MATRICES SUBPLOTS\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "    \n",
    "    # Create a confusion matrix  of raw counts (left subplot)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=None, \n",
    "                                            cmap='gist_gray_r',# Updated cmap\n",
    "                                            values_format=\"d\", \n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[0]);\n",
    "    axes[0].set_title(\"Raw Counts\")\n",
    "    \n",
    "    # Create a confusion matrix with the data with normalize argument \n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=normalize,\n",
    "                                            cmap=cmap, \n",
    "                                            values_format=values_format, #New arg\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[1]);\n",
    "    axes[1].set_title(\"Normalized Confusion Matrix\")\n",
    "    \n",
    "    # Adjust layout and show figure\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return dictionary of classification_report\n",
    "    if output_dict==True:\n",
    "        report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "        return report_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05580fd1-0c3d-4b79-930b-511c7b29ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_network(model, \n",
    "                                    X_train=None, y_train=None, \n",
    "                                    X_test=None, y_test=None,\n",
    "                                    history=None, history_figsize=(6,6),\n",
    "                                    figsize=(6,4), normalize='true',\n",
    "                                    output_dict = False,\n",
    "                                    cmap_train='Blues',\n",
    "                                    cmap_test=\"Reds\",\n",
    "                                    values_format=\".2f\", \n",
    "                                    colorbar=False):\n",
    "    \"\"\"Evaluates a neural network classification task using either\n",
    "    separate X and y arrays or a tensorflow Dataset\n",
    "    \n",
    "    Data Args:\n",
    "        X_train (array, or Dataset)\n",
    "        y_train (array, or None if using a Dataset\n",
    "        X_test (array, or Dataset)\n",
    "        y_test (array, or None if using a Dataset)\n",
    "        history (history object)\n",
    "        \"\"\"\n",
    "    # Plot history, if provided\n",
    "    if history is not None:\n",
    "        plot_history(history, figsize=history_figsize)\n",
    "    ## Adding a Print Header\n",
    "    print(\"\\n\"+'='*80)\n",
    "    print('- Evaluating Network...')\n",
    "    print('='*80)\n",
    "    ## TRAINING DATA EVALUATION\n",
    "    # check if X_train was provided\n",
    "    if X_train is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_train,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_train, y_train_pred = get_true_pred_labels(model, X_train)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_train_pred = model.predict(X_train)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_train = convert_y_to_sklearn_classes(y_train)\n",
    "        y_train_pred = convert_y_to_sklearn_classes(y_train_pred)\n",
    "        \n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_train = classification_metrics(y_train, y_train_pred, \n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_train,\n",
    "                                               values_format=values_format,\n",
    "                                         label='Training Data')\n",
    "        \n",
    "        ## Run model.evaluate         \n",
    "        print(\"\\n- Evaluating Training Data:\")\n",
    "        print(model.evaluate(X_train, return_dict=True))\n",
    "    \n",
    "    # If no X_train, then save empty list for results_train\n",
    "    else:\n",
    "        results_train = []\n",
    "    ## TEST DATA EVALUATION\n",
    "    # check if X_test was provided\n",
    "    if X_test is not None:\n",
    "        ## Check if X_train is a dataset\n",
    "        if hasattr(X_test,'map'):\n",
    "            # If it IS a Datset:\n",
    "            # extract y_train and y_train_pred with helper function\n",
    "            y_test, y_test_pred = get_true_pred_labels(model, X_test)\n",
    "        else:\n",
    "            # Get predictions for training data\n",
    "            y_test_pred = model.predict(X_test)\n",
    "        ## Pass both y-vars through helper compatibility function\n",
    "        y_test = convert_y_to_sklearn_classes(y_test)\n",
    "        y_test_pred = convert_y_to_sklearn_classes(y_test_pred)\n",
    "        \n",
    "        # Call the helper function to obtain regression metrics for training data\n",
    "        results_test = classification_metrics(y_test, y_test_pred, \n",
    "                                         output_dict=True, figsize=figsize,\n",
    "                                             colorbar=colorbar, cmap=cmap_test,\n",
    "                                              values_format=values_format,\n",
    "                                         label='Test Data')\n",
    "        \n",
    "        ## Run model.evaluate         \n",
    "        print(\"\\n- Evaluating Test Data:\")\n",
    "        print(model.evaluate(X_test, return_dict=True))\n",
    "      \n",
    "    # If no X_test, then save empty list for results_test\n",
    "    else:\n",
    "        results_test = []\n",
    "      \n",
    "    # Store results in a dictionary\n",
    "    results_dict = {'train':results_train,\n",
    "                    'test': results_test}\n",
    "    if output_dict == True:\n",
    "        return results_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de11bb-2b55-43da-99a5-02160bb19391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history,figsize=(6,8)):\n",
    "    # Get a unique list of metrics \n",
    "    all_metrics = np.unique([k.replace('val_','') for k in history.history.keys()])\n",
    "    # Plot each metric\n",
    "    n_plots = len(all_metrics)\n",
    "    fig, axes = plt.subplots(nrows=n_plots, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    # Loop through metric names add get an index for the axes\n",
    "    for i, metric in enumerate(all_metrics):\n",
    "        # Get the epochs and metric values\n",
    "        epochs = history.epoch\n",
    "        score = history.history[metric]\n",
    "        # Plot the training results\n",
    "        axes[i].plot(epochs, score, label=metric, marker='.')\n",
    "        # Plot val results (if they exist)\n",
    "        try:\n",
    "            val_score = history.history[f\"val_{metric}\"]\n",
    "            axes[i].plot(epochs, val_score, label=f\"val_{metric}\",marker='.')\n",
    "        except:\n",
    "            pass\n",
    "        finally:\n",
    "            axes[i].legend()\n",
    "            axes[i].set(title=metric, xlabel=\"Epoch\",ylabel=metric)\n",
    "    # Adjust subplots and show\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8cf225-764b-456b-b56a-d57a75c64b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_sklearn_classes(y, verbose=False):\n",
    "    # If already one-dimension\n",
    "    if np.ndim(y)==1:\n",
    "        if verbose:\n",
    "            print(\"- y is 1D, using it as-is.\")\n",
    "        return y\n",
    "        \n",
    "    # If 2 dimensions with more than 1 column:\n",
    "    elif y.shape[1]>1:\n",
    "        if verbose:\n",
    "            print(\"- y is 2D with >1 column. Using argmax for metrics.\")   \n",
    "        return np.argmax(y, axis=1)\n",
    "    \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"y is 2D with 1 column. Using round for metrics.\")\n",
    "        return np.round(y).flatten().astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93443445-82e4-4f49-b39b-b5f60e6a7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_pred_labels(model,ds):\n",
    "    \"\"\"Gets the labels and predicted probabilities from a Tensorflow model and Dataset object.\n",
    "    Adapted from source: https://stackoverflow.com/questions/66386561/keras-classification-report-accuracy-is-different-between-model-predict-accurac\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "    \n",
    "    # Loop through the dataset as a numpy iterator\n",
    "    for images, labels in ds.as_numpy_iterator():\n",
    "        \n",
    "        # Get prediction with batch_size=1\n",
    "        y_probs = model.predict(images, batch_size=1, verbose=0)\n",
    "        # Combine previous labels/preds with new labels/preds\n",
    "        y_true.extend(labels)\n",
    "        y_pred_probs.extend(y_probs)\n",
    "    ## Convert the lists to arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    \n",
    "    return y_true, y_pred_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9a17f-3874-4930-8b2c-1fe67cb556a8",
   "metadata": {},
   "source": [
    "## Importing a .py file from a different folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aeaecf-f239-4169-9ac1-12d3b19fec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding folder above to python path \n",
    "import sys, os\n",
    "sys.path.append( os.path.abspath(\"../\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657dd47-88b1-4f3a-9ce5-8349ff211e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding different sub folder\n",
    "import sys, os\n",
    "sys.path.append( os.path.abspath(\"../Code/\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f960734a-97ce-443f-b487-7ce3d5dd5659",
   "metadata": {},
   "source": [
    "### Auto-Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd493e5-f037-43a9-acb8-f61fb2aee671",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import custom_functions as fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca64d1-fa2e-46ed-832a-3952b9e3a7ed",
   "metadata": {},
   "source": [
    "# Bag of Words Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50b812-8dc2-4811-801d-347399c0179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Then Set Random Seeds\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# Then run the Enable Deterministic Operations Function\n",
    "tf.config.experimental.enable_op_determinism()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811a388-8071-43bc-bcc4-f3e54ae189a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8dc494-779f-4046-93ae-0a5b6fd22c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "​​## Load the autoreload extension\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import custom_functions_lp as fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189e066-3fc9-4957-81e5-98419c6cde97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from your path\n",
    "df = pd.read_csv(\"Data/yelp-restaurant-reviews-CA-2015-2018.csv.gz\", index_col = 'date', parse_dates=['date'])\n",
    "## Remove any non-english reivews\n",
    "df = df.loc[ df['language']=='en'].copy()\n",
    "# Keep only 1,3,5 star reviews\n",
    "df = df.loc[ df['stars'].isin([1,3,5])]\n",
    "# Take a smaller subset\n",
    "df = df.loc['2018']\n",
    "# Set the index\n",
    "df = df.set_index('review_id')\n",
    "df.info()\n",
    "df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18928983-2035-474a-b0eb-dfb383507a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the length of the each text\n",
    "# We will split on each space, and then get the length\n",
    "df['sequence_length'] =df['text'].map( lambda x: len(x.split(\" \")))\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efadd5-20cf-4844-b026-49bb9e8a043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sequence_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee0498-39aa-4654-96e9-51c566d5425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make histogram of sequence lengths\n",
    "ax = df['sequence_length'].hist(bins = 'auto')\n",
    "ax.set_xlabel('Word Count')\n",
    "ax.set_ylabel('Number of Reviews')\n",
    "ax.set_title('Distribution of Sequence Lengths');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b77de9-e6b1-4c52-9891-4c6b1f3114c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a filter to identify reviews less than 400 words\n",
    "filter_short = df['sequence_length']< 400\n",
    "print(f'Number of reviews >400 words is: {len(df) - filter_short.sum()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c6d22-0bca-4415-ba41-65eceaa3365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep short reviews (<400 words)\n",
    "df=  df.loc[filter_short]\n",
    "df.info()\n",
    "df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd531a6-29f3-42e7-8456-17ae8a5c355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "df['stars'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbaa22-acff-44cf-8860-d82f5aa57a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RUS to reduce n to match minority group\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "df_ml,  _ = sampler.fit_resample(df, df['stars'])\n",
    "df_ml['stars'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfaa737-eddc-4745-ae44-61e5bf8556bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X\n",
    "X = df_ml['text'].values\n",
    "# Create a map for targets\n",
    "target_map = {1:0,\n",
    "              3:1,\n",
    "              5:2}\n",
    "# DEfine y and apply the target_map\n",
    "y = df_ml['stars'].map(target_map)\n",
    "y.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8236e24-3a2f-4e86-b67c-7b924a098927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes variable\n",
    "classes = y.unique()\n",
    "classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519fe9f-3ee3-47e1-807b-0a8f1195540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dataset Object\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "# Shuffle dataset\n",
    "ds = ds.shuffle(buffer_size=len(ds),reshuffle_each_iteration=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e851a-bc35-4557-8785-c003329ea408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ratio of the train, validation, test split\n",
    "split_train = .7\n",
    "split_val =  .2\n",
    "split_test =  1 -( split_train + split_val )\n",
    "# Calculate the number of samples for training and validation data \n",
    "n_train_samples =  int(len(ds) * split_train)\n",
    "n_val_samples = int(len(ds) * split_val)\n",
    "n_test_samples = len(ds) -(n_train_samples + n_val_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b04224-2db3-4fbc-8bb5-ddb5389a3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "BATCH_SIZE =32\n",
    "import math\n",
    "# math.ceil will round up\n",
    "# How many batches? \n",
    "n_train_batches = math.ceil(n_train_samples/BATCH_SIZE)\n",
    "n_val_batches = math.ceil(n_val_samples/BATCH_SIZE)\n",
    "n_test_batches = math.ceil(n_test_samples/BATCH_SIZE)\n",
    "print(f\"    - train:\\t{n_train_samples} samples \\t({n_train_batches} batches)\")\n",
    "print(f\"    - val:  \\t{n_val_samples} samples \\t({n_val_batches} batches)\")\n",
    "print(f\"    - test: \\t{n_test_samples} samples \\t({n_test_batches} batches)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a413481-c464-4a92-a273-75b7d2309ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use take and skip to define each set\n",
    "train_ds = ds.take(n_train_samples).batch(batch_size=BATCH_SIZE)\n",
    "# Skip over the training batches and take the validation batches\n",
    "val_ds = ds.skip(n_train_samples).take(n_val_samples).batch(batch_size=BATCH_SIZE)\n",
    "# Skipver the train and validation batches, the remaining are the test batches\n",
    "test_ds = ds.skip(n_train_samples + n_val_samples).batch(batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949c74a-d3de-4f03-963f-d64f83c35c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm len of train_ds\n",
    "len(train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26072e9-d6ee-406a-adc1-831539833856",
   "metadata": {},
   "source": [
    "### Make and fit TextVectorizer layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f66ff-823a-4faa-904e-b796dca36231",
   "metadata": {},
   "source": [
    "### Copy this new function into your custom function .py file for future use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93eeee3-d7c0-4cbb-afe8-a72fe85da11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def make_text_vectorization_layer(train_ds,  max_tokens=None, \n",
    "                                  split='whitespace',\n",
    "                                  standardize=\"lower_and_strip_punctuation\",\n",
    "                                  output_mode=\"int\",\n",
    "                                  output_sequence_length=None,\n",
    "                                  ngrams=None, pad_to_max_tokens=False,\n",
    "                                  verbose=True,\n",
    "                                  **kwargs,\n",
    "                                 ):\n",
    "    # Build the text vectorization layer\n",
    "    text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        standardize=standardize, \n",
    "        output_mode=output_mode,\n",
    "        output_sequence_length=output_sequence_length,\n",
    "        **kwargs\n",
    "    )\n",
    "    # Get just the text from the training data\n",
    "    if isinstance(train_ds, (np.ndarray, list, tuple, pd.Series)):\n",
    "        ds_texts = train_ds\n",
    "    else:\n",
    "        try:\n",
    "            ds_texts = train_ds.map(lambda x, y: x )\n",
    "        except:\n",
    "            ds_texts = train_ds\n",
    "            \n",
    "    # Fit the layer on the training texts\n",
    "    text_vectorizer.adapt(ds_texts)\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        # Print the params\n",
    "        print( \"\\ntf.keras.layers.TextVectorization(\" )\n",
    "        config = text_vectorizer.get_config()\n",
    "        pprint(config,indent=4)\n",
    "        print(\")\")\n",
    "               \n",
    "    # SAVING VOCAB FOR LATER\n",
    "    # Getting list of vocab \n",
    "    vocab = text_vectorizer.get_vocabulary()\n",
    "    # Save dictionaries to look up words from ints \n",
    "    int_to_str  = {idx:word for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    return text_vectorizer, int_to_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19e5b6-66d2-4eee-9396-146affe031fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectorizer layer. Use the custom function to build and fit the vectorizer before using it in model\n",
    "count_vectorizer, count_lookup = make_text_vectorization_layer(train_ds, output_mode='count',\n",
    "                                                                        verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bca9f2-1c69-47a7-968e-b63381e77f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the vocabulary\n",
    "len(count_vectorizer.get_vocabulary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da830aee-60b6-48c8-a042-6fc927850853",
   "metadata": {},
   "source": [
    "### Define a Model Building Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f7652-4bb2-4dc5-8a65-a53b94cfd7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "def build_bow_model(text_vectorization_layer, name=None):\n",
    "    # Build model with pre-trained text_vectorization layer\n",
    "    bow_model = tf.keras.models.Sequential([\n",
    "        text_vectorization_layer], name=name)\n",
    "   \n",
    "    # Add layers\n",
    "    bow_model.add(layers.Dense(32, activation='relu')),\n",
    "    # Output layers\n",
    "    bow_model.add(layers.Dense(len(classes), activation='softmax'))\n",
    "    # Compile model\n",
    "    bow_model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer = optimizers.legacy.Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    bow_model.summary()\n",
    "    return bow_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0d15c-6014-4a40-beaa-0b70ebce50bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below is from Week 4 of Intermediate Machine Learning\n",
    "def get_callbacks(patience=3, monitor='val_accuracy'):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor)\n",
    "    return [early_stop]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e81b07-7d9a-4302-849d-0458ccfc0aeb",
   "metadata": {},
   "source": [
    "### Build, Fit, and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409a3d6-aca0-4c32-bb8c-b7b4df8b2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the adapted TextVectorizer\n",
    "bow_model = build_bow_model(count_vectorizer, name=\"BoW-Counts\")\n",
    "# How many epochs?\n",
    "EPOCHS = 30\n",
    "# Fit the model with callbacks and save history\n",
    "history = bow_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,callbacks=get_callbacks(),\n",
    "    validation_data=val_ds)\n",
    "# Evaluate the model with a custom function\n",
    "fn.evaluate_classification_network(\n",
    "    bow_model, X_train=train_ds, \n",
    "    X_test=test_ds, history=history\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9687f-00a2-4109-9a46-9a05c7a533ad",
   "metadata": {},
   "source": [
    "# Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf824d-4678-4b4f-b0df-4d96c02c3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatically define size of vocab from vectorization layer\n",
    "VOCAB_SIZE = sequence_vectorizer.vocabulary_size()\n",
    "VOCAB_SIZE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5dd598-54c9-46d3-9bcd-2e530d71f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 400\n",
    "SEQUENCE_LENGTH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7a5da-3b27-4184-bfe8-bdf1b1a419f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output dimension\n",
    "EMBED_DIM = 100\n",
    "EMBED_DIM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece7216-1a86-49b9-99c6-b5b73c2bbf30",
   "metadata": {},
   "source": [
    "## Defining an embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192fcbd2-ddb8-4775-80b6-ec676d6d7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an embedding layer\n",
    "embedding_layer = layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67afb4-4a3b-48c7-a8be-6b38c8214e7a",
   "metadata": {},
   "source": [
    "# Intro to RNNs: LSTM models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550a000-b645-49b0-a73b-732d6a8586fa",
   "metadata": {},
   "source": [
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4407f4-44e1-4ed3-8aed-e3bf1b7a962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from your path\n",
    "df = pd.read_csv(\"Data/yelp-restaurant-reviews-CA-2015-2018.csv.gz\", index_col = 'date', parse_dates=['date'])\n",
    "## Remove any non-english reviews\n",
    "df = df.loc[ df['language']=='en'].copy()\n",
    "# Keep only 1,3,5 star reviews\n",
    "df = df.loc[ df['stars'].isin([1,3,5])]\n",
    "# Take a smaller subset\n",
    "df = df.loc['2018']\n",
    "# Set the index\n",
    "df = df.set_index('review_id')\n",
    "# We will split on each space, and then get the length\n",
    "df['sequence_length'] =df['text'].map( lambda x: len(x.split(\" \")))\n",
    "# Define a filter to identify reviews less than 400 words\n",
    "filter_short = df['sequence_length']< 400\n",
    "# Keep short reviews (<400 words)\n",
    "df=  df.loc[filter_short]\n",
    "# Use RUS to reduce n to match minority group\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "df_ml,  _ = sampler.fit_resample(df, df['stars'])\n",
    "df_ml['stars'].value_counts()\n",
    "# Define X\n",
    "X = df_ml['text'].values\n",
    "# Create a map for targets\n",
    "target_map = {1:0,\n",
    "              3:1,\n",
    "              5:2}\n",
    "# Define y and apply the target_map\n",
    "y = df_ml['stars'].map(target_map)\n",
    "# Define classes variable\n",
    "classes = y.unique()\n",
    "classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df14bb3-66db-46f2-8f8f-ed68221e31dd",
   "metadata": {},
   "source": [
    "### Convert to Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45aa93-cf24-4446-8001-12442428ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dataset Object\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "# Shuffle dataset\n",
    "ds = ds.shuffle(buffer_size=len(ds),reshuffle_each_iteration=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c8904-1d6a-4316-a044-db67b804161e",
   "metadata": {},
   "source": [
    "### Train, Validation, and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd9a66-0a34-44ec-832e-0c00ff416bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ratio of the train, validation, test split\n",
    "split_train = .7\n",
    "split_val =  .2\n",
    "split_test =  1 -( split_train + split_val )\n",
    "# Calculate the number of samples for training and validation data \n",
    "n_train_samples =  int(len(ds) * split_train)\n",
    "n_val_samples = int(len(ds) * split_val)\n",
    "n_test_samples = len(ds) -(n_train_samples + n_val_samples)\n",
    "# Set the batch size\n",
    "BATCH_SIZE =32\n",
    "import math\n",
    "# math.ceil will round up\n",
    "# How many batches? \n",
    "n_train_batches = math.ceil(n_train_samples/BATCH_SIZE)\n",
    "n_val_batches = math.ceil(n_val_samples/BATCH_SIZE)\n",
    "n_test_batches = math.ceil(n_test_samples/BATCH_SIZE)\n",
    "print(f\"    - train:\\t{n_train_samples} samples \\t({n_train_batches} batches)\")\n",
    "print(f\"    - val:  \\t{n_val_samples} samples \\t({n_val_batches} batches)\")\n",
    "print(f\"    - test: \\t{n_test_samples} samples \\t({n_test_batches} batches)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c93a1cd-e39e-4e5c-99af-9d90dfae6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use take and skip to define each set\n",
    "train_ds = ds.take(n_train_samples).batch(batch_size=BATCH_SIZE)\n",
    "# Skip over the training batches and take the validation batches\n",
    "val_ds = ds.skip(n_train_samples).take(n_val_samples).batch(batch_size=BATCH_SIZE)\n",
    "# Skipver the train and validation batches, the remaining are the test batches\n",
    "test_ds = ds.skip(n_train_samples + n_val_samples).batch(batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0557a9-f31e-4c04-928d-d2edfc7eb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the number of batches in each\n",
    "print (f' There are {len(train_ds)} training batches.')\n",
    "print (f' There are {len(val_ds)} validation batches.')\n",
    "print (f' There are {len(test_ds)} testing batches.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3dbeee-3fa4-4613-8fa0-04c1d4a5f661",
   "metadata": {},
   "source": [
    "## Creating Sequences with the TextVectorization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c4249-edc3-4e7c-84cf-b47b39709135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequence length as a variable for future use\n",
    "SEQUENCE_LENGTH = 400\n",
    "# Define vectorizer layer. Use the custom function to build and fit the vectorizer before using it in model\n",
    "sequence_vectorizer, vocab_lookup = fn.make_text_vectorization_layer(train_ds, output_mode='int', output_sequence_length = SEQUENCE_LENGTH, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab5220-b1ea-496e-9134-41a731c5a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the vocabulary\n",
    "VOCAB_SIZE = len(sequence_vectorizer.get_vocabulary())\n",
    "VOCAB_SIZE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53113b8-e061-4ea1-99d5-b663976fe401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the vocabulary\n",
    "VOCAB_SIZE = sequence_vectorizer.vocabulary_size()\n",
    "VOCAB_SIZE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b31cc61-44b7-4cd3-9e5e-25ec390bd5a8",
   "metadata": {},
   "source": [
    "## Define Build Function to build an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8bf3fe-c24c-48ef-8056-81b28e19970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for building an LSTM model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "def build_lstm_model(text_vectorization_layer):\n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    lstm_model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    # Add *new* LSTM layer\n",
    "    lstm_model.add(layers.LSTM(128))\n",
    "    # Add output layer\n",
    "    lstm_model.add(layers.Dense(len(classes), activation='softmax'))\n",
    " \n",
    "    # Compile the model\n",
    "    lstm_model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .01), \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    lstm_model.summary()\n",
    "    return lstm_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595c56e-d5a8-4768-98ff-fea23d90cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include callbacks\n",
    "def get_callbacks(patience=3, monitor='val_accuracy'):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor)\n",
    "    return [early_stop]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf9df3-02cb-466f-8632-8d8daadc6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "lstm_model = build_lstm_model(sequence_vectorizer)\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = lstm_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(),\n",
    ")\n",
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    lstm_model, X_train=train_ds, \n",
    "    X_test=test_ds, history=history\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e670b-2356-4c31-9b24-cd7a3e095e04",
   "metadata": {},
   "source": [
    "# GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1545a6a-2ab0-45c5-ac45-e82af2647453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm these values\n",
    "print(f' VOCAB_SIZE: {VOCAB_SIZE}')\n",
    "print(f' EMBED_DIM: {EMBED_DIM}')\n",
    "print(f' SEQUENCE_LENGTH: {SEQUENCE_LENGTH}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf03c88-54c0-451c-a2ff-559b9486d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "def build_gru_model(text_vectorization_layer):\n",
    "                \n",
    "    gru_model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, \n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)])\n",
    "    # Add GRU layer *new*\n",
    "    gru_model.add(layers.GRU(128, return_sequences = True))   \n",
    "    gru_model.add(layers.GlobalMaxPooling1D())\n",
    "    # Output layer\n",
    "    gru_model.add(layers.Dense(len(classes), \n",
    "                              activation='softmax'))\n",
    "        \n",
    "    optimizer = optimizers.legacy.Adam()\n",
    "    gru_model.compile(optimizer=optimizer,  \n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    gru_model.summary()\n",
    "    return gru_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7359e1-0016-478a-b00d-e3b4be417afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include callbacks\n",
    "def get_callbacks(patience=3, monitor='val_accuracy'):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor)\n",
    "    return [early_stop]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bd9dd-1591-4fbd-9328-7ee736a7e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = build_gru_model(sequence_vectorizer)\n",
    "# Fit the model\n",
    "EPOCHS = 30\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(),\n",
    ")\n",
    "# Evaluate the model\n",
    "results = fn.evaluate_classification_network(\n",
    "    model, X_train=train_ds, \n",
    "    X_test=test_ds, history=history);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e3a4c-b6d8-4ca6-9f85-b78c98f627a9",
   "metadata": {},
   "source": [
    "# Stacked RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8eab86-a511-4705-86f1-82ad60c031ea",
   "metadata": {},
   "source": [
    "## Stacked GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a333d1-04c3-4dac-8e5d-e64f96c96ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model_stack_deep_pool(text_vectorization_layer):\n",
    "    \n",
    "    \n",
    "    MAX_TOKENS = text_vectorization_layer.vocabulary_size()\n",
    "        \n",
    "    model = tf.keras.Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)])\n",
    "\n",
    "    # Stack three GrU layers    \n",
    "    model.add(layers.GRU(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.GRU(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(.5))         \n",
    "    # For the final GRU layer, use return_sequences = True for pooling layer\n",
    "    model.add(layers.GRU(128, return_sequences = True)) \n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(len(classes), \n",
    "                              activation='softmax'))\n",
    "\n",
    "              \n",
    "    # Define optimizer\n",
    "    optimizer = optimizers.legacy.Adam()\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=optimizer,  \n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0727139-6d7c-4435-b33b-e335231f226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(patience=3, monitor='val_accuracy'):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor)\n",
    "    return [early_stop]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58bcf5-bb50-4781-9a2b-fa2ff5ac5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = build_gru_model_stack_deep_pool(sequence_vectorizer)\n",
    "\n",
    "EPOCHS = 30\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(),\n",
    ")\n",
    "results = evaluate_classification_network(\n",
    "    model,\n",
    "    X_test=test_ds, history=history\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f108a-6937-4452-b58d-1e95f5483af6",
   "metadata": {},
   "source": [
    "## Stacked RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7733dd-f85b-4527-90d6-4fb918bdb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model_stack_hierarchy_pool(text_vectorization_layer):\n",
    "    \n",
    "    \n",
    "    MAX_TOKENS = text_vectorization_layer.vocabulary_size()\n",
    "        \n",
    "    model = tf.keras.Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)])\n",
    "\n",
    "    # Stack three GrU layers    \n",
    "    model.add(layers.GRU(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.GRU(64, return_sequences=True))\n",
    "    model.add(layers.Dropout(.5))         \n",
    "    # For the final GRU layer, use return_sequences = True for pooling layer\n",
    "    model.add(layers.GRU(32, return_sequences = True)) \n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(len(classes), \n",
    "                              activation='softmax'))\n",
    "\n",
    "              \n",
    "    # Define optimizer\n",
    "    optimizer = optimizers.legacy.Adam()\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=optimizer,  \n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d569c-b9b3-4969-a7ca-f361864f71ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = build_gru_model_stack_hierarchy_pool(sequence_vectorizer)\n",
    "\n",
    "EPOCHS = 30\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(),\n",
    ")\n",
    "results = evaluate_classification_network(\n",
    "    model, \n",
    "    X_test=test_ds, history=history\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b6cc5-0810-4ab2-84b5-f10060ddae5f",
   "metadata": {},
   "source": [
    "## Hybrid Stacked RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc698bfe-752a-4d04-8051-7b4e3f2c90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hybrid_model_deep_pool(text_vectorization_layer):\n",
    "    \n",
    "    \n",
    "    MAX_TOKENS = text_vectorization_layer.vocabulary_size()\n",
    "        \n",
    "    model = tf.keras.Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)])\n",
    "\n",
    "    # Stack   \n",
    "    model.add(layers.GRU(128, return_sequences=True))\n",
    "    model.add(layers.Dropout(.5))     \n",
    "    # For the final layer, use return_sequences = True for pooling layer\n",
    "    model.add(layers.LSTM(128, return_sequences = True)) \n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(len(classes), \n",
    "                              activation='softmax'))\n",
    "\n",
    "              \n",
    "    # Define optimizer\n",
    "    optimizer = optimizers.legacy.Adam()\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=optimizer,  \n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea09466a-56d9-4cca-ba43-99c9a14e36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = build_hybrid_model_deep_pool(sequence_vectorizer)\n",
    "\n",
    "EPOCHS = 30\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(),\n",
    ")\n",
    "results = evaluate_classification_network(\n",
    "    model, \n",
    "    X_test=test_ds, history=history\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c147ec-3292-40ec-ba1d-8ecaea2ba704",
   "metadata": {},
   "source": [
    "# Bi-Directional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3c7a1-fa05-4124-8a27-1df314a93386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm these values\n",
    "print(f' VOCAB_SIZE: {VOCAB_SIZE}')\n",
    "print(f' EMBED_DIM: {EMBED_DIM}')\n",
    "print(f' SEQUENCE_LENGTH: {SEQUENCE_LENGTH}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf7cf4-3db2-4b5c-88b2-ed3f0abd7488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model_bi_pool(text_vectorization_layer):\n",
    "    \n",
    "    gru_model_bi_pool = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                      output_dim=EMBED_DIM,\n",
    "                      input_length=SEQUENCE_LENGTH)])\n",
    "        \n",
    "    # Add bidirectional component to GRU layer \n",
    "    gru_model_bi_pool.add(layers.Bidirectional(layers.GRU(128, return_sequences = True)))\n",
    "    \n",
    "    # Add a pooling layer *new\n",
    "    gru_model_bi_pool.add(layers.GlobalMaxPooling1D())\n",
    "    \n",
    "    # Output layer\n",
    "    gru_model_bi_pool.add(layers.Dense(len(classes), activation='softmax'))\n",
    "                     \n",
    "    optimizer = optimizers.legacy.Adam()\n",
    "    gru_model_bi_pool.compile(optimizer=optimizer, \n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    gru_model_bi_pool.summary()\n",
    "    return gru_model_bi_pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83085c-9724-4d31-bc13-e2e062e2f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include callbacks\n",
    "def get_callbacks(patience=3, monitor='val_accuracy'):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor)\n",
    "    return [early_stop]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db3005-0120-4df3-85fb-f403d35cc2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "​# Build the model\n",
    "model = build_gru_model_bi_pool(sequence_vectorizer)\n",
    "\n",
    "# Fit the model\n",
    "EPOCHS = 30\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(),\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "results = fn.evaluate_classification_network(\n",
    "    model, X_train=train_ds, \n",
    "    X_test=test_ds, history=history);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23689a-367c-41fa-ab32-82c55e5794a3",
   "metadata": {},
   "source": [
    "# Pre-Trained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894033e7-1486-4e8e-b837-c64195d953f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# Load GloVe vectors into a gensim model\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"GloVe2/glove.6B.100d.txt\", binary=False, no_header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a263a-e67c-4cc7-b412-e0d4d56764f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now use `glove_model` to access individual word vectors, similar to a dictionary\n",
    "vector = glove_model['king']\n",
    "vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1799a-2483-453a-b4b5-fbb72b5b701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d16f1-0776-4043-a86d-155e512ced47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similarity between words\n",
    "glove_model.similarity('king', 'queen')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b7260-f3d4-48a4-8e49-428de707ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words\n",
    "glove_model.most_similar('data', topn=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905836d-8d16-4fe3-a714-9b75d241e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform word math\n",
    "result = glove_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cb0c1-75eb-492f-a599-94cb5e4e12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words are included?\n",
    "len(glove_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a5de0-7969-49c7-96c6-3783aca58dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now use `glove_model` to access individual word vectors, similar to a dictionary\n",
    "vector = glove_model['Sarah']\n",
    "vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1219b-c73b-4822-ba58-fe5806bf5f5c",
   "metadata": {},
   "source": [
    "## Making the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15542c1-2e94-4106-a9df-3635fbc4b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(sequence_vectorizer.get_vocabulary())  \n",
    "embedding_dim = 100  # This GloVe model contains vectors of size 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97880f13-8690-4b33-bc59-ac4960f7ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty matrix to hold the word embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca3b37-c726-440c-ab84-da39c15b0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to get each word from vocabulary and get its pre-trained vector from GloVe model\n",
    "for i, word in enumerate(sequence_vectorizer.get_vocabulary()):\n",
    "    try:\n",
    "        embedding_matrix[i] = glove_model[word]\n",
    "    except KeyError:\n",
    "        # If the word is not in the GloVe vocabulary, skip it\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f72796-e646-4886-9806-26ad98d925b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf9e51-9636-47d6-a50c-fa6658bfac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "# Create the embedding layer using the embedding matrix\n",
    "initializer = tf.keras.initializers.Constant(embedding_matrix)\n",
    "glove_embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                            output_dim=embedding_dim,\n",
    "                            embeddings_initializer =initializer,\n",
    "                            trainable=False,  # Keeps the embeddings fixed\n",
    "                            input_length=SEQUENCE_LENGTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39271b8-bd94-4b47-8097-1d2115e27154",
   "metadata": {},
   "source": [
    "### Define Build Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77217047-c430-4b96-96a5-bcf076a72787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "def build_model_glove(text_vectorization_layer, glove_embedding_layer):\n",
    "    \n",
    "    MAX_TOKENS = text_vectorization_layer.vocabulary_size()\n",
    "        \n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        glove_embedding_layer])\n",
    "        \n",
    "    # Bidirectional GRU layer\n",
    "    model.add(layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2)))\n",
    "    # Pooling layer\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(len(classes), activation='softmax'))\n",
    "    # Define the optimizer\n",
    "    optimizer = optimizers.legacy.Adam(learning_rate=0.01)\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer,  \n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81e546-a273-4da4-9c64-5cc31c69f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using restore_best_weights = False \n",
    "def get_callbacks(patience=3, monitor='val_accuracy'):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor, restore_best_weights=False)\n",
    "    return [early_stop]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c883bc-972f-4947-8772-9538f3598e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "baseline_model = build_model_glove(sequence_vectorizer,glove_embedding_layer)\n",
    "EPOCHS = 100\n",
    "# Fit the mdoel\n",
    "history = baseline_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(),\n",
    ")\n",
    "# Evaluate the mdoel\n",
    "results = evaluate_classification_network(\n",
    "    baseline_model, X_train=train_ds, \n",
    "    X_test=test_ds, history=history,\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0798f2-c7b6-435b-990f-75c18d7f8db9",
   "metadata": {},
   "source": [
    "# Saving Tensorflow Models for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40883154-1359-4137-88a5-ce0f4584ed19",
   "metadata": {},
   "source": [
    "## Previous Model Prep Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5cf489-9448-4149-ad78-2751bbb6469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From source: https://keras.io/examples/keras_recipes/reproducibility_recipes/\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Then Set Random Seeds\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# Then run the Enable Deterministic Operations Function\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "# Source: https://discuss.tensorflow.org/t/upgrading-os-to-sonoma-on-my-mac-causing-tensorflow-errors/19846/5\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac604d7e-f866-4be1-b77f-c3b179aea288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d2a1b-441c-4d7b-90e0-d0c28adc2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import custom_functions as fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb29dd-8b7e-4bdb-822e-f5aacbf847f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from your path\n",
    "df = pd.read_csv(\"../NLP/Data/Yelp/subset/yelp-restaurant-reviews-CA-2015-2018.csv.gz\", index_col = 'date', parse_dates=['date'])\n",
    "## Remove any non-english reivews\n",
    "df = df.loc[ df['language']=='en'].copy()\n",
    "# Keep only 1,3,5 star reviews\n",
    "df = df.loc[ df['stars'].isin([1,3,5])]\n",
    "# Take a smaller subset\n",
    "df = df.loc['2018']\n",
    "# Set the index\n",
    "df = df.set_index('review_id')\n",
    "# We will split on each space, and then get the length\n",
    "df['sequence_length'] =df['text'].map( lambda x: len(x.split(\" \")))\n",
    "# Define a filter to identify reviews less than 400 words\n",
    "filter_short = df['sequence_length']< 400\n",
    "# Keep short reviews (<400 words)\n",
    "df=  df.loc[filter_short]\n",
    "# Use RUS to reduce n to match minority group\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "df_ml,  _ = sampler.fit_resample(df, df['stars'])\n",
    "df_ml['stars'].value_counts()\n",
    "# Define X\n",
    "X = df_ml['text'].values\n",
    "# Create a map for targets\n",
    "target_map = {1:0,\n",
    "              3:1,\n",
    "              5:2}\n",
    "# Define y and apply the target_map\n",
    "y = df_ml['stars'].map(target_map)\n",
    "# Define classes variable\n",
    "classes = y.unique()\n",
    "classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d49bc8-b522-4563-9981-e435a9e606db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dataset Object\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "# Shuffle dataset\n",
    "ds = ds.shuffle(buffer_size=len(ds),reshuffle_each_iteration=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e27cd-6998-4652-ae2f-1ce2306c3b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "BATCH_SIZE =32\n",
    "# Set the ratio of the train, validation, test split\n",
    "split_train = .7\n",
    "split_val =  .2\n",
    "split_test =  1 -( split_train + split_val )\n",
    "# Calculate the number of samples for training and validation data \n",
    "n_train_samples =  int(len(ds) * split_train)\n",
    "n_val_samples = int(len(ds) * split_val)\n",
    "n_test_samples = len(ds) -(n_train_samples + n_val_samples)\n",
    "# How many batches? \n",
    "n_train_batches = math.ceil(n_train_samples/BATCH_SIZE)\n",
    "n_val_batches = math.ceil(n_val_samples/BATCH_SIZE)\n",
    "n_test_batches = math.ceil(n_test_samples/BATCH_SIZE)\n",
    "print(f\"    - train:\\t{n_train_samples} samples \\t({n_train_batches} batches)\")\n",
    "print(f\"    - val:  \\t{n_val_samples} samples \\t({n_val_batches} batches)\")\n",
    "print(f\"    - test: \\t{n_test_samples} samples \\t({n_test_batches} batches)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a88d7-6275-4258-8d67-0b8d8d51a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use take and skip to define each set\n",
    "train_ds = ds.take(n_train_samples).batch(batch_size=BATCH_SIZE)\n",
    "# Skip over the training batches and take the validation batches\n",
    "val_ds = ds.skip(n_train_samples).take(n_val_samples).batch(batch_size=BATCH_SIZE)\n",
    "# Skipver the train and validation batches, the remaining are the test batches\n",
    "test_ds = ds.skip(n_train_samples + n_val_samples).batch(batch_size=BATCH_SIZE)\n",
    "# Confirm len of train_ds\n",
    "# Confirm the number of batches in each\n",
    "print (f' There are {len(train_ds)} training batches.')\n",
    "print (f' There are {len(val_ds)} validation batches.')\n",
    "print (f' There are {len(test_ds)} testing batches.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf5e2c-a616-49b5-ae29-697a503327a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequence length as a variable for future use\n",
    "SEQUENCE_LENGTH = 400\n",
    "# Define vectorizer layer. Use the custom function to build and fit the vectorizer before using it in model\n",
    "vectorizer, lookup = fn.make_text_vectorization_layer(train_ds, output_mode='int',output_sequence_length=SEQUENCE_LENGTH, verbose=True)\n",
    "# Check the size of the vocabulary\n",
    "VOCAB_SIZE = len(vectorizer.get_vocabulary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd279d-ad31-4ec6-ad20-37605d36666a",
   "metadata": {},
   "source": [
    "## Model:Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a7704c-c61b-462a-b4d4-edca4e862fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model_bi_pool(text_vectorization_layer):\n",
    "    \n",
    "    gru_model_bi_pool = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                      output_dim=EMBED_DIM,\n",
    "                      input_length=SEQUENCE_LENGTH)])\n",
    "        \n",
    "    # Add bidirectional component to GRU layer \n",
    "    gru_model_bi_pool.add(layers.Bidirectional(layers.GRU(64, return_sequences = True)))\n",
    "    gru_model_bi_pool.add(layers.Dropout(.2))\n",
    "    gru_model_bi_pool.add(layers.Bidirectional(layers.GRU(64, return_sequences = True)))\n",
    "    # Add a pooling layer *new\n",
    "    gru_model_bi_pool.add(layers.GlobalMaxPooling1D())\n",
    "    \n",
    "    # Output layer\n",
    "    gru_model_bi_pool.add(layers.Dense(len(classes), activation='softmax'))\n",
    "                     \n",
    "    optimizer = optimizers.legacy.Adam()\n",
    "    gru_model_bi_pool.compile(optimizer=optimizer, \n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    gru_model_bi_pool.summary()\n",
    "    return gru_model_bi_pool\n",
    "#Include callbacks\n",
    "def get_callbacks(patience=3, monitor='val_accuracy'):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor)\n",
    "    return [early_stop]\n",
    "EMBED_DIM = 100\n",
    "#Build the model\n",
    "model = build_gru_model_bi_pool(vectorizer)\n",
    "# Fit the model\n",
    "EPOCHS = 5#30\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(),\n",
    ")\n",
    "# Evaluate the model\n",
    "results = fn.evaluate_classification_network(\n",
    "    model, X_train=train_ds, \n",
    "    X_test=test_ds, history=history);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62896429-d7c6-44a5-ab68-a69fc418f67e",
   "metadata": {},
   "source": [
    "## Saving Tensorflow Models (for Deployment in a Repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9f779-72e5-4844-a3ad-3bde77c9f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_model = \"Models/gru.keras\"\n",
    "tf.keras.models.save_model(model, fpath_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6facfbb0-989b-4601-92c2-0a0466e1db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(fpath_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7766d-f99d-4df0-aa59-c4c88e069666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to save NLP model as a .keras\n",
    "try:\n",
    "    fpath_model = \"Models/gru.keras\" \n",
    "    tf.keras.models.save_model(model, fpath_model)\n",
    "except Exception as e:\n",
    "    display(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed5d1e-77c5-4316-b598-1c367d98de82",
   "metadata": {},
   "source": [
    "## TensorFlow SavedModel Format (save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69a445-14f9-4e18-b4d6-744e88e0e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model in tf format \n",
    "fpath_model = \"Models/gru/\" # Filename is a folder \n",
    "tf.keras.models.save_model(model, fpath_model, save_format='tf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79bddd-1664-4937-ad53-f596657fffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the file contents of the saved model\n",
    "sorted(os.listdir(fpath_model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84919448-b98c-44e1-990c-fb44cc296c80",
   "metadata": {},
   "source": [
    "### Untraced Functions Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d5a10-12c2-4462-92a5-8dce9dd81fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING:absl:Found untraced functions such as gru_cell_10_layer_call_fn, gru_cell_10_layer_call_and_return_conditional_losses, gru_cell_11_layer_call_fn, gru_cell_11_layer_call_and_return_conditional_losses, gru_cell_13_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de592793-5d67-4a02-9f25-45f0c70384f3",
   "metadata": {},
   "source": [
    "### Loading a Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b91406-5353-40d4-9e9d-9da7273b9cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_network = tf.keras.models.load_model(fpath_model)\n",
    "loaded_network.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f8b4c-06a4-4427-8396-a8c01e44f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = fn.evaluate_classification_network(\n",
    "    loaded_network, X_train=train_ds, \n",
    "    X_test=test_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2877f9-14b6-4e8b-9272-307dc7a040e8",
   "metadata": {},
   "source": [
    "### Saving and Loading Tensorflow Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e6ec6-3cd4-45b2-be45-8ca3e95dde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save:\n",
    "fpath_save_test_ds = \"Data/tf/test/\"\n",
    "tf.data.Dataset.save(test_ds, fpath_save_test_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4e793-2689-4fe5-ab92-52c49463a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Load:\n",
    "test_ds = tf.data.Dataset.load(fpath_save_test_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134475bc-b684-4ccc-9754-d9fc46a0a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the training dataset\n",
    "fpath_save_train_ds = \"Data/tf/train/\"\n",
    "# train_ds.save(path=fpath_save_train_ds)\n",
    "tf.data.Dataset.save(train_ds, fpath_save_train_ds)\n",
    "# Saving the test dataset\n",
    "fpath_save_test_ds = \"Data/tf/test/\"\n",
    "# test_ds.save(path=fpath_save_test_ds)\n",
    "tf.data.Dataset.save(test_ds, fpath_save_test_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de907b57-5506-448f-bc3d-6724d25935fd",
   "metadata": {},
   "source": [
    "### Loading Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0b8b8-350a-4343-b7a3-25f71b1850bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_train = tf.data.TFRecordDataset(fpath_save_train_ds)\n",
    "loaded_train = tf.data.Dataset.load(fpath_save_train_ds)\n",
    "# loaded_test = tf.data.TFRecordDataset(fpath_save_test_ds)\n",
    "loaded_test = tf.data.Dataset.load(fpath_save_test_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51dda3-8442-4701-a74d-b2dbf9a905af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the loaded model with the loaded datasets\n",
    "results = fn.evaluate_classification_network(\n",
    "    loaded_network,X_train=loaded_train, \n",
    "    X_test=loaded_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140f6a4e-51b9-432d-a544-01557292d20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988dbbb8-738e-406b-ab8a-63654c58eb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5eacb-ab22-4371-b496-6163224fc394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8376d-d4f2-4ca1-863d-eff40bc14b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103cd63-eab5-4a30-9da5-267fec59dcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b11f62-ec5e-4ef9-9d9f-e9b975f7423a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac78a19-fda5-4d03-bc47-f7168069bb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69491330-78e5-4e1e-998a-5dc539c4baff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24654f7-4e39-402f-b484-56274109f1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b36634-f9e9-4ba8-8eb1-991e932b12ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fc377-399c-4029-9d6f-41b06c21e07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a4494-4310-4b3e-8015-e60e2fd2fe2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786c27e-b016-4def-a414-f907ca2d4d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7565b00e-4f24-47d2-b13e-8eaaeca3d44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aac061-dc22-453c-aba9-c65090d8bc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
